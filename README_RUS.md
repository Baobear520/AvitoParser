# Daily Parser 

1. [Описание](#описание)

2. [Сервисы проекта](#сервисы-проекта)

3. [Структура проекта](#структура-проекта)

4. [Подробное описание работы главных скриптов](#подробное-описание-работы-главных-скриптов)
5. [Запуск приложения](#запуск-приложения)


## Описание
Проект является функциональной частью приложения, которое оценивает финансовую надежность клиентов с использованием ML и CV.

Он помогает собирать данные для первоначального датасета, используемого для обучения моделей ML и CV, 
а также нацелен на автоматизацию процесса генерации тестовых данных пользователей для приложения на стадиях разработки.

В качестве источника данных для проекта представлены данные, полученные через API https://www.avito.ru.

---
## Сервисы проекта

**Проект использует следующие сервисы:**

Python 3.12.4 (mock_parser) - контейнер для запуска Python-кода

PostgreSQL (postgres_db) - база данных для хранения данных о пользователях и объектах

MinIO S3 (minio) - хранилище для изображений объектов

---
## Структура проекта

**Функционал проекта реализован в 3 основных скриптах**:
- `main_scripts/initial_dataset_collector.py`: фреймворк для извлечения данных для больших датасетов через Avito API с последующим сохранением в CSV-файл или базу данных PostgreSQL.
- `main_scripts/download_photos.py`: фреймворк для скачивания изображений объектов, полученных через Avito API, с сохранением локально на жестком диске/CSV-файл/базу данных/бакет хранилища.
- `main_scripts/mock_user_data_scraper.py`: скрипт, автоматизирующий процесс генерации тестовых данных пользователей для приложения по оценке финансовой надежности.
  Он предназначен для имитации активности пользователей и генерации данных для тестирования или демонстрационных целей. 

**Другие каталоги проекта:**

Каталог **core** содержит набор утилит и классов, используемых в скриптах:

- `core.browsers.py`: классы для управления веб-браузерами
- `core.parsers.py`: классы для парсинга веб-страниц
- `core.utilities`: пакет, содержащий набор утилит и функций:
  - `core.utilities.csv.py`: классы для работы с CSV-файлами через Pandas;
  - `core.utilities.enums.py`: классы для определения перечислений;
  - `core.utilities.minio.py`: классы и функции для работы с бакетами MinIO;
  - `core.utilities.other_functions.py`: коллекция прочих утилитных функций

- `core.exceptions.py`: кастомные исключения
- `core.downloader.py`: классы для скачивания изображений
- `core.settings.py`: настройки конфигурации проекта

Каталог **database** содержит набор классов и функций для взаимодействия с базами данных:
- `database.db.py`: классы для работы с базами данных
- `database.db_schema.py`: схемы базы данных

**tests** - каталог для хранения тестов приложения

**data** - папка для хранения файлов с данными (например, CSV-файлы) по умолчанию

**Другие файлы проекта:**

`.env` - файл для хранения переменных окружения

`dockerfile` - файл для создания Docker-контейнера, используемого в проекте

`docker-compose.yaml` - файл для создания и запуска Docker-контейнеров, используемых в проекте

`requirements.txt` - файл для определения зависимостей Python, используемых в проекте

---
## Подробное описание работы главных скриптов
### mock_user_data_scraper.py
**Скрипт выполняет следующие задачи:**

 - Генерирует тестовых пользователей с dummy - данными (имена, фамилии, номера телефонов, адреса электронной почты), имитируя ежедневное добавление пользователей в приложение;
 - Назначает объекты недвижимости различных категорий (недвижимость, автомобили, электроника, бытовая техника) пользователям с данными, полученными через Avito API или из существующей базы данных/CSV-файла;
 - Сохраняет данные пользователей в базе данных PostgreSQL;
 - Скачивает изображения каждого объекта для нового пользователя и сохраняет их в бакете хранилища;
 - Передает данные изображений в модели CV (данный функционал в настоящее время в разработке).

**Последовательность работы скрипта:**

1. **Инициализация базы данных**: Устанавливается соединение с базой данных PostgreSQL или создается новая (`DB_NAME`), создаются необходимые таблицы и индексы, если они отсутствуют (`DB_SCHEMA`).
2. **Инициализация веб-драйвера**: Инициализируется веб-драйвер для взаимодействия с API источника.
3. **Генерация данных пользователей**: Генерируются случайные данные пользователей (имя пользователя, номер телефона, email и т.д.) с использованием библиотеки `Faker`.
4. **Получение уникальных объектов**: Проверяется, достаточно ли уникальных объектов уже имеется в таблице `unique_records` (таблица для хранения уникальных объектов, еще не назначенных пользователям) для указанной категории. Если условие выполнено, объекты назначаются пользователю, сохраняются в таблице `objects` и удаляются из `unique_records`. Если уникальных объектов недостаточно, происходит получение новых объектов через API.
5. **Получение новых объектов**: Получаются новые объекты из внешнего API (например, Avito), проверяется наличие дубликатов в таблице `objects` и объекты сохраняются в таблице `unique_records` для дальнейшего использования.
6. **Назначение объектов пользователям**: Объекты назначаются пользователям в зависимости от категории и целей пользователя.
7. **Управление базой данных**: Данные о пользователях и объектах вставляются в соответствующие таблицы базы данных в рамках одной транзакции.
8. **Обработка ошибок**: В случае сбоев при получении или обработке данных скрипт повторяет попытки получения объектов до достижения максимального количества неудачных попыток.
9. **Извлечение данных изображений**: Извлекаются данные изображений для каждого объекта из текущей переменной (также поддерживается получение данных из CSV-файла или таблицы базы данных).
10. **Скачивание изображений**: Изображения каждого объекта скачиваются и сохраняются в бакете хранилища (Minio) или локально на жестком диске.
11. _**Передача данных изображений**: Скрипт передает данные изображений в модели CV (в настоящее время в разработке)_

### initial_dataset_collector.py

**Скрипт выполняет следующие задачи:**
- Получает данные из внешнего API (например, Avito) для каждой категории (для текущего проекта они определены в классе `CategoryType` в файле `core.utilities.enums.py`) 
в зависимости от параметров запроса (location, last_stamp, total_goal);
- Формирует датасет из полученных данных;
- Сохраняет его в CSV-файл или таблицу базы данных.

**Последовательность работы скрипта:**

1. **Инициализация веб-драйвера**: Инициализируется веб-драйвер для взаимодействия с исходным API.
2. **Сбор данных**: Получение данных из внешнего API (например, Avito) для каждой категории с заданными параметрами запроса.
3. **Преобразование данных**: Преобразование полученных данных в DataFrame библиотеки pandas.
4. **Сохранение данных**: Сохранение DataFrame в CSV-файл или таблицу базы данных.
5. **Механизм повторных попыток**: Механизм повторных попыток в случае ошибок при обращении к API или отсутствия данных.
6. **Обработка ошибок**: В случае сбоев при получении или обработке данных, а также получении нулевых результатов при парсинге, скрипт повторяет попытки получения данных до достижения максимального количества неудачных попыток.

### download_photos.py

Скрипт использует asyncio и aiohttp для конкурентного скачивания изображений каждого объекта для нового пользователя и сохранения их в бакете Minio. Изображения сохраняются в бакете с уникальным ключом (`user_id/object_id/image_counter.jpg`) для каждого изображения.

Скрипт также поддерживает функциональность скачивания изображений из CSV-файла или таблицы базы данных с последующим сохранением их на жестком диске или в отдельной базе данных.

---
## Запуск приложения
### Требования

Для запуска приложения необходимо, чтобы на вашей системе был установлен Python 3.9 или выше.

### Installation

1. Клонируйте репозиторий и перейдите в директорию `AvitoParser`:

```bash
git clone https://github.com/Baobear520/AvitoParser
cd AvitoParser
```

2. Приложение использует Selenium и Chrome веб-драйвер для взаимодействия с исходным API и парсинга данных для обхода немедленной блокировки. Убедитесь, что на вашей системе установлен Chrome.

3. Установка Chrome веб-драйвера будет выполнена автоматически во время процесса инициализации (встроено в класс BaseParser в модуле core/browsers.py).

### Настройка окружения
Создайте файл `.env` и настройте переменные окружения для проекта:

```bash
DB_NAME=your_db_name
DB_USER=your_db_user
DB_PASSWORD=your_db_password
DB_HOST=your_db_host
DB_PORT=your_db_port
MINIO_HOST=your_minio_host
MINIO_ROOT_USER=your_minio_access_key
MINIO_ROOT_PASSWORD=your_minio_secret_key
MINIO_ENDPOINT=your_minio_port
```
Другие важные, но не конфиденциальные настройки определены в файле `settings.py`

### Запуск через Docker
Для запуска приложения через Docker необходимо установить Docker на вашей системе. Файл `dockerfile`, находящийся в корневой директории репозитория, содержит инструкции для сборки Docker-образа приложения. 
Файл docker-compose.yaml содержит инструкции для определения Docker-контейнеров, используемых в приложении:

- postgres_db: база данных PostgreSQL.
- mock_parser: Python-приложение.
- minio: бакет хранилища MinIO.

Контейнеры будут использовать переменные окружения, определенные в файле `.env` и необходимые для запуска скрипта парсера и подключения к базе данных и хранилищу MinIO.

Для запуска контейнеров выполните:

```bash
docker compose up -d
```
Приложение будет доступно по адресу http://localhost:8000

### Запуск локально
Создайте виртуальное окружение, активируйте его и установите необходимые зависимости из файла `requirements.txt`:

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Запустите скрипт mock_user_data_scraper.py из вашей IDE (например, PyCharm) или через командную строку:

```bash
python mock_user_data_scraper.py
```
Логи работы скрипта будут выводиться в терминал (логирование с записью в файл находится на стадии разработки).